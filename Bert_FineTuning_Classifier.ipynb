{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new enviroment was created to run this experiment\n",
    "# Pip install pytorch_pretrained_bert\n",
    "# The model class and dataloader class called base class provided by https://github.com/huggingface/pytorch-pretrained-BERT\n",
    "# Pytorch v1.0\n",
    "# Run with 2 GPUs\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTraining, BertPreTrainedModel, BertModel, BertConfig\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "import pdb\n",
    "from tqdm import tqdm, trange\n",
    "import sys\n",
    "# import apex\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=Path('Data/experiment')\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "PATH = Path('Data/')\n",
    "PATH.mkdir(exist_ok= True)\n",
    "\n",
    "OUT_PATH = Path('Output/')\n",
    "OUT_PATH.mkdir(exist_ok = True)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change these parameters to run experiments\n",
    "\n",
    "args = {\n",
    "    \"train_size\": -1,\n",
    "    \"val_size\": -1,\n",
    "    \"full_data_dir\": DATA_PATH,\n",
    "    \"data_dir\": PATH,\n",
    "    \"task_name\": \"Sentiment_multiclass\",\n",
    "    \"bert_model\": 'bert-base-cased',\n",
    "    \"output_dir\": OUT_PATH,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"do_lower_case\": False,\n",
    "    \"train_batch_size\": 16,\n",
    "    \"eval_batch_size\": 16,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"num_train_epochs\": 4.0,\n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"no_cuda\": False,\n",
    "    \"local_rank\": -1,\n",
    "    \"seed\": 421,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"fp16\": False,\n",
    "    \"loss_scale\": 128,\n",
    "    'n_embd': 768,\n",
    "    \"classifier_dropout\": 0.3,\n",
    "    \"heads_per_class\": 1,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"use_softmax\": True,\n",
    "    \"layer_sizes\": [2048,1024, 5 ],\n",
    "    \"train_file\": 'train_5000_train.csv',\n",
    "    \"val_file\": 'train_5000_val.csv',\n",
    "    \"test_file\": 'test_onehot.csv',\n",
    "    'train_on_full': True,\n",
    "    'train_full_file': 'train_5000.csv',\n",
    "    'model_fname': 'BertFineTuneClf',\n",
    "    'test_pred_file': 'test_using_5000.csv'\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMultiClassClf(BertPreTrainedModel):\n",
    "    \"\"\"BERT model for multi-class classification.\n",
    "    This module is composed of the BERT model with multiple linear layers on top of\n",
    "    the pooled output.\n",
    "    Each linear layers has PReLU activation and dropout. \n",
    "    \n",
    "    Params:\n",
    "        config: a BertConfig class instance with the configuration to build a new model.\n",
    "        num_labels: the number of classes for the classifier\n",
    "    Inputs:\n",
    "        input_ids: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        token_type_ids: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        attention_mask: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        labels: labels for the classification output: torch.LongTensor of shape [batch_size, num_labels].\n",
    "    Outputs:\n",
    "        if labels is not None:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if labels is None:\n",
    "            Outputs the classification logits of shape [batch_size, num_labels].\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        super(BertMultiClassClf, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        #self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.classifier = MultiLayerClassifier(input_layer_size=args['n_embd'], \n",
    "                                               layer_sizes = args['layer_sizes'], \n",
    "                                               dropout=args['classifier_dropout'], \n",
    "                                                softmax=args['use_softmax'])\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        out = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(out.view(-1, self.num_labels), torch.max(labels,1)[1])\n",
    "            return loss\n",
    "        else:\n",
    "            return out\n",
    "        \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "class MultiLayerClassifier(nn.Module):\n",
    "    def __init__(self, input_layer_size, layer_sizes, dropout=0.1, init_dropout=False,\n",
    "                 softmax=True):\n",
    "        super(MultiLayerClassifier, self).__init__()\n",
    "        self.nclasses = num_labels\n",
    "        self.device = -1\n",
    "        self.final = num_labels\n",
    "        self.dropout = dropout\n",
    "        self.nonlinearity = nn.PReLU()  #Use PReLU for activation\n",
    "        self.layer_sizes = [input_layer_size] + list(map(int, layer_sizes))\n",
    "        self.final = self.layer_sizes[-1]\n",
    "        # layer_sizes are sizes of the input and hidden layers, so the final 1 is assumed.\n",
    "        layer_list = []\n",
    "        if init_dropout:\n",
    "            layer_list.extend([nn.Dropout(p=self.dropout)])\n",
    "#         layer_list.extend(list(chain.from_iterable(\n",
    "#             [[nn.Linear(self.layer_sizes[i], self.layer_sizes[i+1]),\n",
    "#               nn.BatchNorm1d(self.layer_sizes[i+1]), self.nonlinearity, \n",
    "#               nn.Dropout(p=self.dropout)] for i in range(len(self.layer_sizes) - 2)])))\n",
    "        layer_list.extend(list(chain.from_iterable(\n",
    "            [[nn.Linear(self.layer_sizes[i], self.layer_sizes[i+1]),\n",
    "              self.nonlinearity, \n",
    "              nn.Dropout(p=self.dropout)] for i in range(len(self.layer_sizes) - 2)])))\n",
    "        self.final_layer = nn.Linear(*self.layer_sizes[-2:])\n",
    "        extend_list = [self.final_layer]\n",
    "        if not softmax:\n",
    "            extend_list += [nn.Sigmoid()]\n",
    "        layer_list.extend(extend_list)\n",
    "\n",
    "        self.model = nn.Sequential(*layer_list)\n",
    "        self.softmax = softmax\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        out = self.model(X).float()\n",
    " \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, labels=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            labels: (Optional) [string]. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir, filename):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_val_examples(self, data_dir, filename):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the validation set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_test_examples(self, data_dir, filename):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n",
    "        raise NotImplementedError() \n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelTextProcessor(DataProcessor):\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels = None\n",
    "    \n",
    "    \n",
    "    def get_train_examples(self, data_dir, filename):\n",
    "        data_df = pd.read_csv(os.path.join(data_dir, filename))\n",
    "        return self._create_examples(data_df, \"train\")\n",
    "\n",
    "        \n",
    "    def get_val_examples(self, data_dir,filename):\n",
    "        data_df = pd.read_csv(os.path.join(data_dir, filename))\n",
    "        return self._create_examples(data_df, \"val\")\n",
    "        \n",
    "    \n",
    "    def get_test_examples(self, data_dir, filename):\n",
    "        data_df = pd.read_csv(os.path.join(data_dir, filename))\n",
    "        return self._create_examples(data_df, \"test\")\n",
    "\n",
    "\n",
    "    def get_labels(self):\n",
    "        \n",
    "        return ['rating_1','rating_2','rating_3','rating_4','rating_5']\n",
    "\n",
    "    def _create_examples(self, df, set_type, labels_available=True):\n",
    "        \"\"\"Creates examples for the training and val sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, row) in enumerate(df.values):\n",
    "            guid = i\n",
    "            text_a = row[0]\n",
    "            if labels_available:\n",
    "                labels = row[1:]\n",
    "            else:\n",
    "                labels = []\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, labels=labels))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        \n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[:(max_seq_length -2)]\n",
    "            \n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        \n",
    "        labels_ids = []\n",
    "        for label in example.labels:           \n",
    "            labels_ids.append(float(label))\n",
    "\n",
    "        if ex_index < 0:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %s)\" % (example.labels, labels_ids))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_ids=labels_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = fbeta_score(y_true=labels, y_pred=preds, beta = 1,average='weighted' )\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1-macro\": f1,\n",
    "        \"acc_and_f1-macro\": (acc + f1) / 2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = {\n",
    "    \"sentiment_multiclass\": MultiLabelTextProcessor\n",
    "}\n",
    "\n",
    "# Setup GPU parameters\n",
    "\n",
    "if args[\"local_rank\"] == -1 or args[\"no_cuda\"]:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args[\"no_cuda\"] else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "\n",
    "else:\n",
    "    torch.cuda.set_device(args['local_rank'])\n",
    "    device = torch.device(\"cuda\", args['local_rank'])\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args['local_rank'] != -1), args['fp16']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['train_batch_size'] = int(args['train_batch_size'] / args['gradient_accumulation_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up task\n",
    "task_name = args['task_name'].lower()\n",
    "\n",
    "if task_name not in processors:\n",
    "    raise ValueError(\"Task not found: %s\" % (task_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = processors[task_name](args['data_dir'])\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rating_1', 'rating_2', 'rating_3', 'rating_4', 'rating_5']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer from Bert model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training set\n",
    "train_examples = None\n",
    "num_train_steps = None\n",
    "if args['do_train']:\n",
    "    if args['train_on_full']:\n",
    "        train_examples = processor.get_train_examples(args['full_data_dir'], args['train_full_file'])\n",
    "    else:\n",
    "        train_examples = processor.get_train_examples(args['full_data_dir'],args['train_file'])\n",
    "    \n",
    "    num_train_steps = int(\n",
    "        len(train_examples) / args['train_batch_size'] / args['gradient_accumulation_steps'] * args['num_train_epochs'])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare model\n",
    "def get_model():\n",
    "    model = BertMultiClassClf.from_pretrained('bert-base-cased', num_labels = num_labels)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "if args['fp16']:\n",
    "    model.half()\n",
    "model.to(device)\n",
    "if args['local_rank'] != -1:\n",
    "    try:\n",
    "        from apex.parallel import DistributedDataParallel as DDP\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "    model = DDP(model)\n",
    "elif n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BertMultiClassClf(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28996, 768)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (classifier): MultiLayerClassifier(\n",
       "      (nonlinearity): PReLU(num_parameters=1)\n",
       "      (final_layer): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (model): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Dropout(p=0.3)\n",
       "        (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "        (4): PReLU(num_parameters=1)\n",
       "        (5): Dropout(p=0.3)\n",
       "        (6): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Learning Rate\n",
    "from torch.optim.lr_scheduler import _LRScheduler, Optimizer\n",
    "\n",
    "class SlantedTriangularLR(_LRScheduler):\n",
    "    \"\"\"\n",
    "    Implement the slanted triangular learning rate schedule used for ULMFiT.\n",
    "    \n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        lr_ratio (float): ratio of minimum to maximum learning rate\n",
    "        max_val (float): highest learning rate\n",
    "        cut_frac (float): fraction of iterations we increase the LR\n",
    "        num_iters (int): number of epochs times the number of updates per epoch\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, lr_ratio=32, max_val=0.01, cut_frac=0.1, num_iters=1000):\n",
    "        self.optimizer = optimizer\n",
    "        self.min_val = max_val / lr_ratio\n",
    "        self.max_val = max_val\n",
    "        self.cut = num_iters * cut_frac \n",
    "        self.end_triangle_iter = num_iters\n",
    "        self.num_iters = 0\n",
    "        self.lr_func = self.create_lr_func()\n",
    "\n",
    "\n",
    "    def create_lr_func(self):\n",
    "        lr_range = self.max_val - self.min_val\n",
    "\n",
    "        up_slope = lr_range / self.cut\n",
    "        up_intercept = self.min_val\n",
    "        down_slope = -lr_range / (self.end_triangle_iter - self.cut)\n",
    "        down_intercept = -down_slope * self.cut + self.max_val\n",
    "\n",
    "        def lr_func():\n",
    "            if self.num_iters <= self.cut:\n",
    "                return up_slope * self.num_iters + up_intercept\n",
    "            else:\n",
    "                return down_slope * self.num_iters + down_intercept\n",
    "\n",
    "        return lr_func\n",
    "\n",
    "    def step(self, step_num=None):\n",
    "        if step_num is None:\n",
    "            step_num = self.num_iters + 1\n",
    "        self.num_iters = step_num\n",
    "        new_lr = self.lr_func()\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "t_total = num_train_steps\n",
    "if args['local_rank'] != -1:\n",
    "    t_total = t_total // torch.distributed.get_world_size()\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=args['learning_rate'],\n",
    "                         warmup=args['warmup_proportion'],\n",
    "                         t_total=t_total)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Evaluation step\n",
    "eval_examples = processor.get_val_examples(args['full_data_dir'], args['val_file'])\n",
    "\n",
    "def eval():\n",
    "    \n",
    "    eval_features = convert_examples_to_features(\n",
    "        eval_examples, label_list, args['max_seq_length'], tokenizer)\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in eval_features], dtype=torch.long)\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    \n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    preds = []\n",
    "    \n",
    "    for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "        \n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        tmp_eval_loss = loss_fct(logits.view(-1, num_labels), torch.max(label_ids,1)[1])\n",
    "        \n",
    "        if len(preds) == 0:\n",
    "            preds.append(logits.detach().cpu().numpy())\n",
    "        else:\n",
    "            preds[0] = np.append(\n",
    "                        preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "\n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = preds[0]\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    all_label_ids2 = np.argmax(all_label_ids.numpy(), axis = 1)\n",
    "    eval_accuracy = simple_accuracy(preds, all_label_ids2)\n",
    "    eval_acc_f1 = acc_and_f1(preds, all_label_ids2)\n",
    "\n",
    "    \n",
    "\n",
    "    result = {'eval_loss': eval_loss,\n",
    "              'eval_accuracy': eval_accuracy,\n",
    "             'f1-macro': eval_acc_f1['f1-macro']}\n",
    "\n",
    "    output_eval_file = os.path.join(OUT_PATH, \"eval_results.txt\")\n",
    "    if os.path.exists(output_eval_file):\n",
    "        cmd = 'a'\n",
    "    else: cmd = 'w'\n",
    "        \n",
    "    with open(output_eval_file, cmd) as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        writer.write(\"Model \"+ args['model_fname']+\"\\n\")\n",
    "        writer.write(\"Layers \"+ ' + '.join([str(i) for i in args['layer_sizes']])+'\\n')\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = convert_examples_to_features(\n",
    "    train_examples, label_list, args['max_seq_length'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "logger.info(\"  Batch size = %d\", args['train_batch_size'])\n",
    "logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_ids for f in train_features], dtype=torch.long)\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "if args['local_rank'] == -1:\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "else:\n",
    "    train_sampler = DistributedSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args['train_batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_per_epoch = len(train_features)\n",
    "num_iters = iters_per_epoch * args['num_train_epochs']\n",
    "\n",
    "scheduler = SlantedTriangularLR(optimizer, lr_ratio=32, \n",
    "                                max_val=0.01, cut_frac=0.1, num_iters=num_iters)\n",
    "\n",
    "\n",
    "def fit(num_epocs=args['num_train_epochs']):\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    for i_ in tqdm(range(int(num_epocs)), desc=\"Epoch\"):\n",
    "\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "            \n",
    "            loss.backward()\n",
    "                \n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "#                 scheduler.optimizer.step()\n",
    "                # modify learning rate with special warm up BERT uses\n",
    "                lr_this_step = args['learning_rate'] * warmup_linear(global_step/t_total, args['warmup_proportion'])\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_this_step\n",
    "#                 scheduler.step()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "        logger.info('Loss after epoc {}'.format(tr_loss / nb_tr_steps))\n",
    "        if args['train_on_full'] == False:\n",
    "            logger.info('Eval after epoc {}'.format(i_+1))\n",
    "            eval()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.freeze_bert_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.unfreeze_bert_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Save model\n",
    "# Save a trained model\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join(PYTORCH_PRETRAINED_BERT_CACHE, \"finetuned_pytorch_model_full.bin\")\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "\n",
    "# #Load pretrained model\n",
    "# model_state_dict = torch.load(output_model_file)\n",
    "# model = BertMultiClassClf.from_pretrained(args['bert_model'], num_labels = num_labels, state_dict=model_state_dict)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae2b83d1cee4be5b67bd18551250340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=63, style=ProgressStyle(description_width='i"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9228694003725809,\n",
       " 'eval_accuracy': 0.64,\n",
       " 'f1-macro': 0.6428485577571229}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = eval()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, path, test_filename):\n",
    "    predict_processor = MultiLabelTextProcessor(path)\n",
    "    test_examples = predict_processor.get_test_examples(path, test_filename)\n",
    "    \n",
    "    # Hold input data for returning it \n",
    "    input_data = [{ 'id': input_example.guid, 'text': input_example.text_a } for input_example in test_examples]\n",
    "\n",
    "    test_features = convert_examples_to_features(\n",
    "        test_examples, label_list, args['max_seq_length'], tokenizer)\n",
    "    \n",
    "    logger.info(\"***** Running prediction *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(test_examples))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in test_features], dtype=torch.long)\n",
    "    \n",
    "    test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    \n",
    "    # Run prediction for full data\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=args['eval_batch_size'])\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    preds = []\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(test_dataloader, desc=\"Prediction Iteration\")):\n",
    "        input_ids, input_mask, segment_ids,label_ids = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "        \n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        tmp_test_loss = loss_fct(logits.view(-1, num_labels), torch.max(label_ids,1)[1])\n",
    "        \n",
    "        if len(preds) == 0:\n",
    "            preds.append(logits.detach().cpu().numpy())\n",
    "        else:\n",
    "            preds[0] = np.append(\n",
    "                        preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "        test_loss += tmp_test_loss.mean().item()\n",
    "        \n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    test_loss = test_loss / nb_eval_steps\n",
    "    \n",
    "    preds = preds[0]\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    all_label_ids2 = np.argmax(all_label_ids.numpy(), axis = 1)\n",
    "    test_accuracy = simple_accuracy(preds, all_label_ids2)\n",
    "    test_acc_f1 = acc_and_f1(preds, all_label_ids2)\n",
    "\n",
    "    \n",
    "\n",
    "    result = {'test_loss': test_loss,\n",
    "              'test_accuracy': test_accuracy,\n",
    "             'test_f1-macro': test_acc_f1['f1-macro']}\n",
    "\n",
    "    output_test_file = os.path.join(OUT_PATH, \"test_results.txt\")\n",
    "    if os.path.exists(output_test_file):\n",
    "        cmd = 'a'\n",
    "    else: cmd = 'w'\n",
    "        \n",
    "    with open(output_test_file, cmd) as writer:\n",
    "        logger.info(\"***** Test results *****\")\n",
    "        writer.write(\"*******Test Results *******\\n\")\n",
    "        writer.write(\"Model \"+ args['model_fname']+\"\\n\")\n",
    "        writer.write(\"Layers \"+ ' + '.join([str(i) for i in args['layer_sizes']])+'\\n')\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "            writer.write(\"------------------------------------\\n\")\n",
    "    \n",
    "    test_df = pd.DataFrame(input_data) \n",
    "    test_df['prediction'] = preds\n",
    "    return result, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11adc99bfb1e430299678562ee554bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Prediction Iteration', max=3125, style=ProgressStyle(descript"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hnguyen1/project/bert/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "result, test_df = predict(model, args['data_dir'], args['test_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 1.6622514208984376,\n",
       " 'test_accuracy': 0.1994,\n",
       " 'test_f1-macro': 0.10847427651888963}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save prediction result: \n",
    "test_df.to_csv(OUT_PATH/args['test_pred_file'], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
